## 信息熵与交叉熵
> [【直观详解】信息熵、交叉熵和相对熵](https://charlesliuyx.github.io/2017/09/11/%E4%BB%80%E4%B9%88%E6%98%AF%E4%BF%A1%E6%81%AF%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8C%E7%9B%B8%E5%AF%B9%E7%86%B5/)
[为什么交叉熵（cross-entropy）可以用于计算代价](https://www.zhihu.com/question/65288314)

### 信息熵
**其实信息熵是香农信息量（$log\frac{1}{p}$）的期望（均值），它不是针对每条信息，而是针对整个不确定性结果集而言，信息熵越大，事件不确定性就越大。单条信息只能从某种程度上影响结果集概率的分布**。信息熵定义：
$$H(P) = \sum_{i} P(i)log_{a} \frac{1}{P(i)} = -\sum_{i}P(i)log_{a} P(i)$$
$P_{i}$ 表示第 i 个事件发生得概率，总的来说信息熵其实从某种意义上反映了**信息量存储下来需要多少存储空间**。
总结为：根据真实分布，我们能够找到一个最优策略，以**最小的代价消除系统的不确定性**（比如编码），而这个代价的大小就是**信息熵**。
### 相对熵/KL散度
KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同，记为 $D(P||Q) = H(P,Q) - H(P)$，也称之为 `KL散度`，离散事件数学公式如下：
$$D_{KL}(P \| Q) = \sum_i P(i)log_{a} \frac{P(i)}{Q(i)} = \sum_i P(i)[logP(x) - log Q(x)]$$
当 `P(i) = Q(i)` 的时候，该值为0，深度学习过程也是一个降低该值的过程，**该值越低，训练出来的概率 Q 越接近样本集概率 P，即越准确**，或者可以理解为相对熵一把标尺，用来衡量两个函数是否相似，相似就是 0。即，**相对熵 = 某个策略的交叉熵 - 信息熵**（根据系统真实分布计算而得的信息熵，为最优策略），**当熵为常量时，交叉熵与KL散度相等**（推导公式可参考第二个链接）。
### 交叉熵 cross-entroy
交叉熵是由信息熵而得来的，和 KL 散度关系密切，拓展用在机器学习/深度学习中作损失函数。假定在确定性更大的概率分布情况下，用更不确定的存储策略来计算，比如使用 `P` 的概率乘上 `Q` 的存储因子，套用信息熵公式：
$$H(P) = \sum_{i} P(i)log_{a} \frac{1}{Q(i)} = -\sum_{i}P(i)log_{a} Q(i)$$
用预测概率 p 分布,去编码真实标签 q 的分布,得到的信息量。**交叉熵，用来衡量在给定的真实分布下，使用非真实分布指定的策略消除系统的不确定性所需要付出努力的大小**。总的来说，我们的目的是：让熵尽可能小，即存储空间小（消除系统的不确定的努力小）。**交叉熵的一些性质：**
+ 非负。
+ 和 KL 散度相同，交叉熵也不具备对称性，即 `H(P,Q)≠H(Q,P)`。
+ 交叉熵主要用于描述两个事件之间的相互关系，对同一个分布求交叉熵等于对其求熵

### 为什么交叉熵可以用作代价
从数学上来理解就是，为了让学到的模型分布更接近真实数据的分布，我们需要最小化模型数据分布与训练数据之间的 `KL 散度`，而因为训练数据的分布是固定的，因此最小化 `KL 散度`等价于最小化交叉熵，而且交叉熵计算更简单，所以机器/深度学习中常用交叉熵 `cross-entroy` 作为分类问题的损失函数。

### KL散度与cross-entropy的关系
+ `KL` 散度和交叉熵在特定条件下等价
+ $D_{KL}(P||Q) = H(P,Q) - H(P)$
## KNN算法
`K` 近邻算法（KNN）是一种基本分类和回归方法。`KNN` 算法的核心思想是如果一个样本在特征空间中的 `k` 个最相邻的样本中的大多数属于一个类别，那该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分类样本所属的类别。 如下图：

![KNN算法](../images/KNN算法.png)

在 `KNN` 中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离，公式如下：

![欧式距离计算](../images/欧式距离.png)

同时，KNN通过依据 k 个对象中占优的类别进行决策，而不是单一的对象类别决策，这两点就是KNN算法的优势。
### k 值的选取
+ `k` 值较小，模型会变得复杂，容易发生过拟合
+ `k` 值较大，模型比较简单，容易欠拟合

所以 `k` 值得选取也是一种调参？
### KNN算法思路
`KNN` 思想就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前 `K` 个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：
1. 计算测试数据与各个训练数据之间的距离；
2. 按照距离的递增关系进行排序；
3. 选取距离最小的 `K` 个点；
4. 确定前 `K` 个点所在类别的出现频率；
5. 返回前 `K` 个点中出现频率最高的类别作为测试数据的预测分类。
## 支持向量机算法
> [机器学习中的算法(2)-支持向量机(SVM)基础](https://www.cnblogs.com/leftnoteasy/archive/2011/05/02/basic-of-svm.html)
### 支持向量机简述
+ 支持向量机（Support Vector Machines, SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使其成为实质上的非线性分类器。
+ `SVM` 的学习策略是找到最大间隔（两个异类支持向量到超平面的距离之和 $\gamma = \frac{2}{||w}$ 称为“间隔”），可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。
+ `SVM` 的最优化算法是求解凸二次规划的最优化算法。

### SVM 基本型
$$min \frac{1}{2}||w||^2$$
$$s.t. y_{i}(w^Tx_i + b) \geq 1, i = 1,2,...,m$$ 
SVM 的最优化算法是一个凸二次规划（convex quadratic programming）问题，对上式使用拉格朗日乘子法可转化为对偶问题，并优化求解。

### 对偶问题求解
对 `SVM` 基本型公式的每条约束添加拉格朗日乘子 $\alpha_i \geq 0$，则式子的拉格朗日函数如下：
$$L(w,b,a) = \frac 1 2||w||^2 - \sum{i=1}{n} \alpha_i (y_i(w^Tx_i+b) - 1)$$
经过推导（参考机器学习西瓜书），可得 SVM 基本型的对偶问题：
$$\max\limits_{\alpha} \sum_{i=1}^{m}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m}\alpha_i \alpha_j y_i y_j x^{T}_{i} x_j$$
$$s.t. \sum_{i=1}^{m} = \alpha_{i}y_{i} = 0$$
$$\alpha_{i}\geq 0, i=1,2,...,m$$
继续优化该问题，有 `SMO` 方法，SMO 的基本思路是先固定 $\alpha_i$ 之外的的所有参数，然后求 $\alpha_i$ 上的极值。
## K-means聚类算法
### 分类与聚类算法
+ 分类简单来说，就是根据文本的特征或属性，划分到已有的类别中。也就是说，这些类别是已知的，通过对已知分类的数据进行训练和学习，找到这些不同类的特征，再对未分类的数据进行分类。
+ 聚类，就是你压根不知道数据会分为几类，通过聚类分析将数据或者说用户聚合成几个群体，那就是聚类了。聚类不需要对数据进行训练和学习。
+ 分类属于监督学习，聚类属于无监督学习。常见的分类比如决策树分类算法、贝叶斯分类算法等聚类的算法最基本的有系统聚类，K-means 均值聚类。
### K-mean聚类算法概述
聚类的目的是找到每个样本 x 潜在的类别 y，并将同类别 y 的样本 x 放在一起。在聚类问题中，假定训练样本是${x^1,...,x^m}$，每个 $x^i \in R^n$，没有 `y`。`K-means` 算法是将样本聚类成 k 个簇（cluster），算法过程如下：
1. 随机选取 k 个聚类中心（cluster centroids）为 $\mu_1, \mu_1,...,\mu_k \in R^n$。
2. 重复下面过程，直到质心不变或者变化很小：
    + 对于每一个样例 `i` ，计算其所属类别：$$c^i = arg min \underset{j}{min}||x^i - \mu_j||^2$$
    + 对于每一个类 `j`，重新计算该类的质心：$$\mu_j = \frac {\sum_{i=1}^{m}1\{c^i = j\}x^i} { \sum_{i=1}^{m}1\{c^i = j\}}$$

`K` 是我们事先给定的聚类数，$c^i$ 代表样例 `i` 与 `k` 个类中距离最近的那个类，$c^i$ 的值是 `1` 到 `k` 中的一个。质心 $\mu_j$ 代表我们对属于同一个类的样本中心点的猜测。

## 参考资料
[K-means聚类算法](https://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html)

